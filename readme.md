# GO_CIPO_BACKEND

# Продакшн проект с 3 элементами:

- реализация Rest.API сервиса по учету товаров магазина Cipo и лендингу
- парсер выгрузки из 1С - 2-х XML с описаниями товаров и остатками
- раздача статики - картинок для товаров и новостей

# Основные элементы проекта:

- Postgres в стандартном контейнере
- SERVER - rest.api на стеке Go 1.23
  -- Fiber v3 beta, возможность запуска в PREFORK (?), многоядерный режим
  -- PGX v5 в native режиме и scany для сканирования
- MIGRATOR - бинарник на основе golang-migrate и SQL-скриптов
- PARSER - для парсинга XML-файлов из директории Input, имена файлов задаются в .env
- SEEDER - для записи в БД минимально необходимого для парсера, запускать после миграций

# Текущие проблемы и ограничения

[v] prefork-режим в SERVER работает нестабильно - исправлено через сборку с dumb-init, entrypoint с ним, cmd exec

- сбор метрик в SERVER в случае больше 1 горутины или prefork - работает некорректно, так как собирает каждый поток независимо
- пакет для сканирования ответа от PGX (scany) использует рефлексию, что снижает производительность на 20% от теоретической

# Как запускать PARSER из запущенного контейнера:

`docker exec go_cipo_backend-cipo_backend_server-1 ./PARSER`
при этом в контейнере в папке /app должны быть из хоста примонтированы папки:

- /assets - туда выкладываются картинки
- /input - оттуда читаются XML для загрузки

# TODO

- [v] нужны ли Entity-структуры когда есть DTO-структуры?
  Варианты:

  1. использовать один тип DTO для сквозной передачи от storage-функции до handler. Тогда нужно сканировать результат Select сразу в DTO.
  2. соблюдать изоляцию слоев - в storage-функции сканировать в Entity, далее в сервисе перекладывать в другую структуру с типом DTO. Тогда написать вспомогательную функцию для копирования из одной структуру в другую.
     Что решено:
  3. изолируем слои в свои структуры, из БД сканируем в Entity,
  4. в сервисе создаем DTO путем копирования структуры из Entity (github.com/jinzhu/copier).

- [v] fiber - валидация входящих вопросов - выполнено по ручкам user
- [v] парсер целиком
- [v] clearDB - бинарник для чистки в БД таблицы qnt_price_registry, которая разрастается при каждом парсинге
- [v] docker билд контейнера и пуш в docker hub
- [v] healthcheck в сервере и docker compose
- [v] prom-client для мониторинга
- [ ] e2e-тесты и юнит-тесты (парсера целиком и отдельных функций)
- [ ] стейдж-развертывание из контейнеров в docker swarm, helm
- [ ] перенос создания prometheus-registry на уровень главного процесса (в main), чтобы корректно собирать данные из всех горутин и процессов prefork
- [ ] ендпойнт metrics вынести на уровень главного процесса (в main) через стандартный http и с отдельным портом
- [v] поэкспериментировать с prefork
- [ ] отдельный бинарник PRICER, для формирования прайс-листа Kaspi в XML. В параметры запуска передавать критерии отбора товаров через флаги или путь до YAML с критериями. Сам XML публиковать в папке со статикой (фиксированный URL)
